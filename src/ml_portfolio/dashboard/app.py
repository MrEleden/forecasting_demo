"""
Streamlit dashboard for model comparison and exploration.

Run with:
    streamlit run src/ml_portfolio/dashboard/app.py
"""

import json
import sys
from pathlib import Path

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


# Page config
st.set_page_config(
    page_title="ML Forecasting Dashboard",
    page_icon="ðŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS
st.markdown(
    """
<style>
    .main {
        padding: 0rem 1rem;
    }
    .stMetric {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 5px;
    }
</style>
""",
    unsafe_allow_html=True,
)


@st.cache_data
def load_benchmark_data(filepath: str = "results/benchmarks/benchmark_results.json"):
    """Load benchmark results from file."""
    try:
        with open(filepath, "r") as f:
            data = json.load(f)
        return pd.DataFrame(data["results"])
    except FileNotFoundError:
        return None


@st.cache_data
def load_predictions(filepath: str):
    """Load prediction data."""
    try:
        return pd.read_csv(filepath)
    except FileNotFoundError:
        return None


def main():
    """Main dashboard application."""

    # Title and description
    st.title("ðŸ“ˆ ML Forecasting Model Dashboard")
    st.markdown("Compare forecasting models, explore predictions, and analyze performance")

    # Sidebar
    st.sidebar.header("Navigation")
    page = st.sidebar.radio(
        "Select Page",
        ["Overview", "Model Comparison", "Predictions Explorer", "Benchmark Results"],
    )

    # ========================================================================
    # OVERVIEW PAGE
    # ========================================================================
    if page == "Overview":
        st.header("Overview")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(label="Total Models", value="6+", delta="Production Ready")

        with col2:
            st.metric(label="Datasets", value="4", delta="Multi-domain")

        with col3:
            st.metric(label="Test Coverage", value="78%", delta="+15% this week")

        st.markdown("---")

        # Features overview
        st.subheader("Available Features")

        features_col1, features_col2 = st.columns(2)

        with features_col1:
            st.markdown(
                """
            #### Models
            - Statistical: ARIMA, Prophet, SARIMAX
            - Machine Learning: LightGBM, CatBoost, XGBoost
            - Deep Learning: LSTM, TCN, Transformer
            - Ensemble: Stacking, Voting
            """
            )

        with features_col2:
            st.markdown(
                """
            #### Capabilities
            - Probabilistic forecasting (quantile regression)
            - Automated hyperparameter tuning (Optuna)
            - Data validation (Pandera)
            - FastAPI serving endpoint
            """
            )

        st.markdown("---")

        # Quick stats
        st.subheader("Quick Stats")

        # Load benchmark data if available
        df_benchmark = load_benchmark_data()

        if df_benchmark is not None:
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                best_model = df_benchmark.loc[df_benchmark["mape"].idxmin(), "model_name"]
                st.metric("Best Model (MAPE)", best_model)

            with col2:
                best_mape = df_benchmark["mape"].min()
                st.metric("Best MAPE", f"{best_mape:.4f}")

            with col3:
                avg_train_time = df_benchmark["training_time"].mean()
                st.metric("Avg Training Time", f"{avg_train_time:.2f}s")

            with col4:
                total_runs = len(df_benchmark)
                st.metric("Total Benchmark Runs", total_runs)
        else:
            st.info("Run benchmark suite to see statistics: `python scripts/run_benchmark.py`")

    # ========================================================================
    # MODEL COMPARISON PAGE
    # ========================================================================
    elif page == "Model Comparison":
        st.header("Model Comparison")

        df_benchmark = load_benchmark_data()

        if df_benchmark is None:
            st.warning("No benchmark results found. Please run: `python scripts/run_benchmark.py`")
            return

        # Filters
        st.sidebar.subheader("Filters")

        available_datasets = df_benchmark["dataset_name"].unique()
        selected_dataset = st.sidebar.selectbox("Select Dataset", ["All"] + list(available_datasets))

        if selected_dataset != "All":
            df_filtered = df_benchmark[df_benchmark["dataset_name"] == selected_dataset]
        else:
            df_filtered = df_benchmark

        available_models = df_filtered["model_name"].unique()
        selected_models = st.sidebar.multiselect("Select Models", available_models, default=list(available_models))

        if selected_models:
            df_filtered = df_filtered[df_filtered["model_name"].isin(selected_models)]

        # Metrics selection
        metric = st.sidebar.selectbox("Primary Metric", ["mape", "rmse", "mae"])

        # Display data table
        st.subheader("Results Table")
        st.dataframe(
            df_filtered[
                [
                    "model_name",
                    "dataset_name",
                    "mape",
                    "rmse",
                    "mae",
                    "training_time",
                    "prediction_time",
                ]
            ].style.highlight_min(subset=["mape", "rmse", "mae"], color="lightgreen")
        )

        st.markdown("---")

        # Visualizations
        col1, col2 = st.columns(2)

        with col1:
            st.subheader(f"{metric.upper()} Comparison")

            # Bar chart
            fig = px.bar(
                df_filtered.groupby("model_name")[metric].mean().reset_index(),
                x="model_name",
                y=metric,
                color=metric,
                color_continuous_scale="RdYlGn_r",
                title=f"Average {metric.upper()} by Model",
            )
            fig.update_layout(xaxis_title="Model", yaxis_title=metric.upper())
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            st.subheader("Training Time vs Accuracy")

            # Scatter plot
            fig = px.scatter(
                df_filtered,
                x="training_time",
                y=metric,
                color="model_name",
                size="n_samples",
                hover_data=["dataset_name"],
                title="Trade-off: Training Time vs Accuracy",
            )
            fig.update_layout(xaxis_title="Training Time (seconds)", yaxis_title=metric.upper())
            st.plotly_chart(fig, use_container_width=True)

        # Box plot for metric distribution
        st.subheader("Metric Distribution")
        fig = px.box(
            df_filtered,
            x="model_name",
            y=metric,
            color="model_name",
            title=f"Distribution of {metric.upper()} Scores",
        )
        fig.update_layout(xaxis_title="Model", yaxis_title=metric.upper())
        st.plotly_chart(fig, use_container_width=True)

        # Rankings
        st.subheader("Model Rankings")
        rankings = df_filtered.groupby("model_name")[metric].mean().sort_values().reset_index()
        rankings["rank"] = range(1, len(rankings) + 1)
        rankings.columns = ["Model", metric.upper(), "Rank"]

        st.dataframe(rankings.style.background_gradient(subset=[metric.upper()], cmap="RdYlGn_r"))

    # ========================================================================
    # PREDICTIONS EXPLORER PAGE
    # ========================================================================
    elif page == "Predictions Explorer":
        st.header("Predictions Explorer")

        # File uploader for prediction data
        uploaded_file = st.file_uploader(
            "Upload prediction CSV", type=["csv"], help="Upload CSV with actual and predicted values"
        )

        if uploaded_file is not None:
            df_pred = pd.read_csv(uploaded_file)

            # Try to auto-detect columns
            date_col = None
            actual_col = None
            pred_col = None

            for col in df_pred.columns:
                if "date" in col.lower() or "time" in col.lower():
                    date_col = col
                elif "actual" in col.lower() or "true" in col.lower():
                    actual_col = col
                elif "pred" in col.lower() or "forecast" in col.lower():
                    pred_col = col

            # Column selection
            col1, col2, col3 = st.columns(3)

            with col1:
                date_col = st.selectbox(
                    "Date Column", df_pred.columns, index=df_pred.columns.get_loc(date_col) if date_col else 0
                )

            with col2:
                actual_col = st.selectbox(
                    "Actual Values Column",
                    df_pred.columns,
                    index=df_pred.columns.get_loc(actual_col) if actual_col else 1,
                )

            with col3:
                pred_col = st.selectbox(
                    "Predicted Values Column",
                    df_pred.columns,
                    index=df_pred.columns.get_loc(pred_col) if pred_col else 2,
                )

            # Convert date column
            df_pred[date_col] = pd.to_datetime(df_pred[date_col])

            # Calculate metrics
            actual_values = df_pred[actual_col].values
            pred_values = df_pred[pred_col].values

            from ml_portfolio.evaluation.metrics import mae, mape, rmse

            mape_score = mape(actual_values, pred_values)
            rmse_score = rmse(actual_values, pred_values)
            mae_score = mae(actual_values, pred_values)

            # Display metrics
            col1, col2, col3 = st.columns(3)
            col1.metric("MAPE", f"{mape_score:.4f}")
            col2.metric("RMSE", f"{rmse_score:.2f}")
            col3.metric("MAE", f"{mae_score:.2f}")

            st.markdown("---")

            # Time series plot
            st.subheader("Time Series Comparison")

            fig = go.Figure()

            fig.add_trace(
                go.Scatter(
                    x=df_pred[date_col],
                    y=df_pred[actual_col],
                    mode="lines",
                    name="Actual",
                    line=dict(color="blue", width=2),
                )
            )

            fig.add_trace(
                go.Scatter(
                    x=df_pred[date_col],
                    y=df_pred[pred_col],
                    mode="lines",
                    name="Predicted",
                    line=dict(color="red", width=2, dash="dash"),
                )
            )

            fig.update_layout(
                title="Actual vs Predicted Values",
                xaxis_title="Date",
                yaxis_title="Value",
                hovermode="x unified",
            )

            st.plotly_chart(fig, use_container_width=True)

            # Residual analysis
            st.subheader("Residual Analysis")

            residuals = actual_values - pred_values
            df_pred["residuals"] = residuals

            col1, col2 = st.columns(2)

            with col1:
                # Residuals over time
                fig = px.scatter(
                    df_pred,
                    x=date_col,
                    y="residuals",
                    title="Residuals Over Time",
                    labels={"residuals": "Residual"},
                )
                fig.add_hline(y=0, line_dash="dash", line_color="red")
                st.plotly_chart(fig, use_container_width=True)

            with col2:
                # Residual histogram
                fig = px.histogram(
                    df_pred,
                    x="residuals",
                    nbins=30,
                    title="Residual Distribution",
                    labels={"residuals": "Residual"},
                )
                st.plotly_chart(fig, use_container_width=True)

        else:
            st.info("Upload a CSV file with actual and predicted values to explore predictions")

    # ========================================================================
    # BENCHMARK RESULTS PAGE
    # ========================================================================
    elif page == "Benchmark Results":
        st.header("Benchmark Results")

        df_benchmark = load_benchmark_data()

        if df_benchmark is None:
            st.warning("No benchmark results found. Please run: `python scripts/run_benchmark.py`")
            st.code("python scripts/run_benchmark.py", language="bash")
            return

        # Summary statistics
        st.subheader("Summary Statistics")

        summary = df_benchmark.groupby("model_name").agg(
            {
                "mape": ["mean", "std", "min", "max"],
                "rmse": ["mean", "std", "min", "max"],
                "mae": ["mean", "std", "min", "max"],
                "training_time": ["mean", "std"],
            }
        )

        st.dataframe(summary.style.format("{:.4f}"))

        st.markdown("---")

        # Detailed results
        st.subheader("Detailed Results")

        # Add filters
        col1, col2 = st.columns(2)

        with col1:
            sort_by = st.selectbox("Sort by", ["mape", "rmse", "mae", "training_time"])

        with col2:
            ascending = st.checkbox("Ascending", value=True)

        df_sorted = df_benchmark.sort_values(by=sort_by, ascending=ascending)

        st.dataframe(
            df_sorted,
            use_container_width=True,
            column_config={
                "mape": st.column_config.NumberColumn("MAPE", format="%.4f"),
                "rmse": st.column_config.NumberColumn("RMSE", format="%.4f"),
                "mae": st.column_config.NumberColumn("MAE", format="%.4f"),
                "training_time": st.column_config.NumberColumn("Training Time (s)", format="%.2f"),
                "prediction_time": st.column_config.NumberColumn("Prediction Time (s)", format="%.4f"),
            },
        )

        # Download button
        csv = df_benchmark.to_csv(index=False)
        st.download_button(
            label="Download Results as CSV",
            data=csv,
            file_name="benchmark_results.csv",
            mime="text/csv",
        )


if __name__ == "__main__":
    main()
