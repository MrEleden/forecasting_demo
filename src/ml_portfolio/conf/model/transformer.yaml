# @package _global_

# Transformer Model Configuration
# Deep learning attention-based time series model
# Dataloader, engine, optimizer, and scheduler specified via defaults

defaults:
  - /dataloader: pytorch
  - /engine: pytorch
  - /optimizer: adam
  - /scheduler: cosine

model:
  # Model instantiation
  _target_: ml_portfolio.models.pytorch.transformer.TransformerForecaster

  # Architecture
  input_size: 1  # Will be set based on dataset
  output_size: 1  # Forecast horizon
  d_model: 128  # Embedding dimension
  nhead: 8  # Number of attention heads (must divide d_model)
  num_encoder_layers: 3
  num_decoder_layers: 3
  dim_feedforward: 512  # FFN hidden dimension
  dropout: 0.1

  # Training (will be used by engine)
  learning_rate: 0.001

# Optuna hyperparameter search space
optuna:
  # Architecture search
  d_model_options: [64, 128, 256, 512]

  nhead_options: [4, 8, 16]  # Must divide d_model

  num_encoder_layers_min: 1
  num_encoder_layers_max: 6

  num_decoder_layers_min: 1
  num_decoder_layers_max: 6

  dim_feedforward_min: 128
  dim_feedforward_max: 2048

  # Regularization
  dropout_min: 0.0
  dropout_max: 0.5

  # Training
  learning_rate_min: 0.0001
  learning_rate_max: 0.01
  learning_rate_log: true

  batch_size_options: [16, 32, 64, 128]

  num_epochs_min: 50
  num_epochs_max: 200

# Model metadata
metadata:
  family: deep_learning
  type: transformer
  framework: pytorch
  supports_multivariate: true
  supports_exog: true
  attention_based: true
  parallelizable: true
  good_for:
    - long_sequences
    - complex_patterns
    - multivariate
    - attention_interpretation
