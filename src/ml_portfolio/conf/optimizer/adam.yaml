# Adam Optimizer Configuration
# Adaptive learning rate optimizer - good default for most cases

_target_: torch.optim.Adam

lr: 0.001  # Learning rate
betas: [0.9, 0.999]  # Coefficients for computing running averages
eps: 1.0e-08  # Term added for numerical stability
weight_decay: 0.0  # L2 regularization (0 = no regularization)
amsgrad: false  # Whether to use AMSGrad variant

# params will be passed by the engine automatically
