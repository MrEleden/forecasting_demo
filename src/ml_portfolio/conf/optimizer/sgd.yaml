# SGD optimizer configuration (PyTorch)
# Stochastic Gradient Descent with momentum

_target_: torch.optim.SGD

lr: 0.01  # Learning rate
momentum: 0.9  # Momentum factor
weight_decay: 0.0  # L2 regularization
dampening: 0.0
nesterov: false  # Whether to use Nesterov momentum
