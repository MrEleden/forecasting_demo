# SGD Optimizer Configuration
# Stochastic Gradient Descent with momentum

_target_: torch.optim.SGD

lr: 0.01  # Learning rate
momentum: 0.9  # Momentum factor
weight_decay: 0.0  # L2 regularization
dampening: 0.0  # Dampening for momentum
nesterov: false  # Whether to use Nesterov momentum

# params will be passed by the engine automatically
