{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Walmart Model Training Workflow\n",
    "\n",
    "A guided notebook to prepare features, train a baseline model, and evaluate forecasts for Walmart weekly sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Load engineered features (or regenerate them on demand).\n",
    "- Produce time-aware train/validation/test splits for a selected store.\n",
    "- Fit a RandomForest baseline and report MAE, RMSE, and MAPE.\n",
    "- Persist the trained artifacts for downstream pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"axes.titlesize\": 14, \"axes.labelsize\": 12})\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists() and PROJECT_ROOT != PROJECT_ROOT.parent:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "CONF_DIR = SRC_DIR / \"ml_portfolio\" / \"conf\"\n",
    "FEATURE_CONFIG_NAME = \"walmart_full\"\n",
    "FEATURE_CONFIG_PATH = CONF_DIR / \"feature_engineering\" / f\"{FEATURE_CONFIG_NAME}.yaml\"\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"projects\" / \"retail_sales_walmart\" / \"data\"\n",
    "RAW_DATA_PATH = DATA_DIR / \"raw\" / \"Walmart.csv\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "FEATURE_CACHE = PROCESSED_DIR / f\"walmart_features_{FEATURE_CONFIG_NAME}.parquet\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"projects\" / \"retail_sales_walmart\" / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw dataset path: {RAW_DATA_PATH}\")\n",
    "print(f\"Models directory: {MODEL_DIR}\")\n",
    "print(f\"Feature config: {FEATURE_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Feature preparation helper\n",
    "\n",
    "Hydra configs drive the same static feature engineering used by `train.py`. The cell below loads the raw dataset, applies the configured static pipeline, and caches the resulting table under `data/processed/`. Set `force_refresh=True` to rebuild the parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_table(force_refresh: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Create or load engineered features using the shared Hydra pipeline.\"\"\"\n",
    "    processed_path = FEATURE_CACHE\n",
    "\n",
    "    if processed_path.exists() and not force_refresh:\n",
    "        df_cached = pd.read_parquet(processed_path)\n",
    "        df_cached[\"Date\"] = pd.to_datetime(df_cached[\"Date\"])\n",
    "        return df_cached\n",
    "\n",
    "    feature_cfg = OmegaConf.load(FEATURE_CONFIG_PATH)\n",
    "    static_cfg = feature_cfg.get(\"static\")\n",
    "    if static_cfg is None:\n",
    "        raise ValueError(f\"No static pipeline defined in {FEATURE_CONFIG_PATH}\")\n",
    "\n",
    "    static_pipeline = instantiate(static_cfg)\n",
    "\n",
    "    df_raw = pd.read_csv(RAW_DATA_PATH)\n",
    "    df_raw = df_raw.sort_values([\"Store\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    features_df = static_pipeline.engineer_features(df_raw)\n",
    "    features_df[\"Date\"] = pd.to_datetime(features_df[\"Date\"], dayfirst=True, format=\"mixed\")\n",
    "\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    features_df.to_parquet(processed_path, index=False)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "features_df = prepare_feature_table(force_refresh=False)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Train / validation / test splits\n",
    "\n",
    "We create chronological splits for a single store (default: store 1) to avoid leakage across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STORE = 1\n",
    "store_df = (\n",
    "    features_df[features_df[\"Store\"] == TARGET_STORE]\n",
    "    .sort_values(\"Date\")\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    " )\n",
    "\n",
    "total_rows = len(store_df)\n",
    "train_idx = int(total_rows * 0.7)\n",
    "val_idx = int(total_rows * 0.85)\n",
    "\n",
    "feature_cols = [col for col in store_df.columns if col not in [\"Weekly_Sales\", \"Date\", \"Store\"]]\n",
    "\n",
    "X_train = store_df.iloc[:train_idx][feature_cols].astype(float).values\n",
    "y_train = store_df.iloc[:train_idx][\"Weekly_Sales\"].values\n",
    "\n",
    "X_val = store_df.iloc[train_idx:val_idx][feature_cols].astype(float).values\n",
    "y_val = store_df.iloc[train_idx:val_idx][\"Weekly_Sales\"].values\n",
    "\n",
    "X_test = store_df.iloc[val_idx:][feature_cols].astype(float).values\n",
    "y_test = store_df.iloc[val_idx:][\"Weekly_Sales\"].values\n",
    "test_dates = store_df.iloc[val_idx:][\"Date\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Samples â€” train: {len(y_train)}, val: {len(y_val)}, test: {len(y_test)}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Train a RandomForest baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\"Model fitted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_split(name: str, y_true, y_pred) -> dict:\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    denom = np.where(y_true != 0, y_true, np.finfo(float).eps)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
    "    return {\"split\": name, \"MAE\": mae, \"RMSE\": rmse, \"MAPE(%)\": mape}\n",
    "\n",
    "pred_train = rf_model.predict(X_train_scaled)\n",
    "pred_val = rf_model.predict(X_val_scaled)\n",
    "pred_test = rf_model.predict(X_test_scaled)\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    [\n",
    "        evaluate_split(\"train\", y_train, pred_train),\n",
    "        evaluate_split(\"validation\", y_val, pred_val),\n",
    "        evaluate_split(\"test\", y_test, pred_test),\n",
    "    ]\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({\"Date\": test_dates, \"Actual\": y_test, \"Predicted\": pred_test})\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(data=test_results, x=\"Date\", y=\"Actual\", label=\"Actual\")\n",
    "sns.lineplot(data=test_results, x=\"Date\", y=\"Predicted\", label=\"Predicted\")\n",
    "plt.title(\"RandomForest predictions vs actuals (test split)\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Weekly Sales ($)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(rf_model.feature_importances_, index=feature_cols)\n",
    "top_features = feature_importance.sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=top_features.values, y=top_features.index, orient=\"h\")\n",
    "plt.title(\"Top feature importances (RandomForest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "top_features.to_frame(name=\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = MODEL_DIR / f\"random_forest_store{TARGET_STORE}.joblib\"\n",
    "SCALER_PATH = MODEL_DIR / f\"random_forest_store{TARGET_STORE}_scaler.joblib\"\n",
    "METRICS_PATH = MODEL_DIR / f\"random_forest_store{TARGET_STORE}_metrics.parquet\"\n",
    "\n",
    "joblib.dump(rf_model, MODEL_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "metrics_df.to_parquet(METRICS_PATH, index=False)\n",
    "\n",
    "print(f\"Model saved to {MODEL_PATH}\")\n",
    "print(f\"Scaler saved to {SCALER_PATH}\")\n",
    "print(f\"Metrics logged to {METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
