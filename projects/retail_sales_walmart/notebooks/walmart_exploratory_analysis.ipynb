{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Walmart Retail Sales Exploratory Analysis\n",
    "\n",
    "Exploratory notebook for the Walmart weekly sales dataset used throughout the forecasting portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Resolve project paths so the notebook works from the repo root or the project folder.\n",
    "- Load the raw Walmart sales data and inspect its schema and data quality.\n",
    "- Generate exploratory visualisations for overall, per-store, and seasonal behaviour.\n",
    "- Preview the static feature engineering pipeline that powers the training workflow.\n",
    "- Persist an enriched feature table into `data/processed/` for downstream experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 6), \"axes.titlesize\": 14, \"axes.labelsize\": 12})\n",
    "\n",
    "def resolve_project_dir(project_name: str = \"retail_sales_walmart\") -> Path:\n",
    "    \"\"\"Locate the Walmart project directory regardless of launch location.\"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "\n",
    "    # Common entry points: repo root, project folder, notebooks folder\n",
    "    if (cwd / \"projects\" / project_name).exists():\n",
    "        return cwd / \"projects\" / project_name\n",
    "\n",
    "    if cwd.name == project_name and (cwd / \"data\").exists():\n",
    "        return cwd\n",
    "\n",
    "    if cwd.name == \"notebooks\" and (cwd.parent / \"data\").exists():\n",
    "        return cwd.parent\n",
    "\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \"projects\" / project_name).exists():\n",
    "            return parent / \"projects\" / project_name\n",
    "        if parent.name == project_name and (parent / \"data\").exists():\n",
    "            return parent\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Unable to locate project directory. Run from the repo root or ensure the \"\n",
    "        \"current working directory contains the retail_sales_walmart project.\"\n",
    "    )\n",
    "\n",
    "PROJECT_DIR: Final[Path] = resolve_project_dir()\n",
    "DATA_DIR: Final[Path] = PROJECT_DIR / \"data\"\n",
    "RAW_PATH: Final[Path] = DATA_DIR / \"raw\" / \"Walmart.csv\"\n",
    "PROCESSED_DIR: Final[Path] = DATA_DIR / \"processed\"\n",
    "FEATURE_CACHE: Final[Path] = PROCESSED_DIR / \"walmart_features_exploratory.parquet\"\n",
    "CONF_DIR: Final[Path] = PROJECT_DIR.parents[1] / \"src\" / \"ml_portfolio\" / \"conf\"\n",
    "FEATURE_CONFIG_NAME: Final[str] = \"walmart_full\"\n",
    "FEATURE_CONFIG_PATH: Final[Path] = CONF_DIR / \"feature_engineering\" / f\"{FEATURE_CONFIG_NAME}.yaml\"\n",
    "\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print(f\"Raw data path: {RAW_PATH}\")\n",
    "print(f\"Feature config: {FEATURE_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load and inspect the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(RAW_PATH)\n",
    "row_count, col_count = df_raw.shape\n",
    "print(f\"Rows: {row_count:,} | Columns: {col_count}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Data quality overview\n",
    "We compute summary statistics, missing-value counts, and duplicate checks over the canonical Store-Date key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_raw.describe(include=\"all\").transpose())\n",
    "missing_summary = (\n",
    "    df_raw.isna().sum().rename(\"missing_count\").to_frame()\n",
    "    .assign(missing_pct=lambda d: (d[\"missing_count\"] / row_count) * 100)\n",
    ")\n",
    "display(missing_summary.sort_values(\"missing_pct\", ascending=False))\n",
    "duplicate_rows = df_raw.duplicated(subset=[\"Store\", \"Date\"]).sum()\n",
    "print(f\"Duplicate [Store, Date] pairs: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Seasonal and store-level behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d-%m-%Y\")\n",
    "df.sort_values([\"Store\", \"Date\"], inplace=True)\n",
    "df[\"Week\"] = df[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "df[\"Month\"] = df[\"Date\"].dt.month\n",
    "df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
    "df[\"DayOfYear\"] = df[\"Date\"].dt.dayofyear\n",
    "\n",
    "df[\"Week_sin\"] = np.sin(2 * math.pi * df[\"Week\"] / 52)\n",
    "df[\"Week_cos\"] = np.cos(2 * math.pi * df[\"Week\"] / 52)\n",
    "df[\"Month_sin\"] = np.sin(2 * math.pi * df[\"Month\"] / 12)\n",
    "df[\"Month_cos\"] = np.cos(2 * math.pi * df[\"Month\"] / 12)\n",
    "\n",
    "store_summary = (\n",
    "    df.groupby(\"Store\")[\"Weekly_Sales\"]\n",
    "    .agg([\"mean\", \"median\", \"std\", \"max\"])\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "display(store_summary.head(10))\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(data=df, x=\"Date\", y=\"Weekly_Sales\", hue=\"Store\", legend=False)\n",
    "plt.title(\"Weekly sales per store\")\n",
    "plt.ylabel(\"Weekly Sales (USD)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Holiday effect snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_totals = (\n",
    "    df.groupby([\"Holiday_Flag\", \"Store\"])\n",
    "    [\"Weekly_Sales\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sns.barplot(data=holiday_totals, x=\"Store\", y=\"Weekly_Sales\", hue=\"Holiday_Flag\")\n",
    "plt.title(\"Average weekly sales by holiday flag\")\n",
    "plt.ylabel(\"Weekly Sales (USD)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 3. Reusing the Hydra static feature pipeline\n",
    "The training script relies on `feature_engineering/walmart_full.yaml`. We can instantiate that static pipeline here to ensure parity between exploration and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cfg = OmegaConf.load(FEATURE_CONFIG_PATH)\n",
    "static_cfg = feature_cfg.get(\"static\")\n",
    "static_pipeline = instantiate(static_cfg)\n",
    "features_df = static_pipeline.engineer_features(df_raw.copy())\n",
    "features_df[\"Date\"] = pd.to_datetime(features_df[\"Date\"], dayfirst=True, format=\"mixed\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Feature importance proxy\n",
    "A quick tree-based model gives a sense of which engineered signals matter most on a single store sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_example = features_df[features_df[\"Store\"] == 1].sort_values(\"Date\")\n",
    "feature_cols = [col for col in store_example.columns if col not in [\"Weekly_Sales\", \"Date\", \"Store\"]]\n",
    "X = store_example[feature_cols].astype(float).values\n",
    "y = store_example[\"Weekly_Sales\"].values\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=importances.values, y=importances.index, orient=\"h\")\n",
    "plt.title(\"Top engineered features (Store 1)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "importances.to_frame(name=\"importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 4. Persist enriched features\n",
    "Cache the engineered table so training notebooks or scripts can reuse the same features without recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "features_df.to_parquet(FEATURE_CACHE, index=False)\n",
    "print(f\"Feature table saved to {FEATURE_CACHE}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
