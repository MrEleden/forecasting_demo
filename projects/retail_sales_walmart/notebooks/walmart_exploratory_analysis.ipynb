{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8972df9",
   "metadata": {},
   "source": [
    "# Walmart Retail Sales Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49358d4",
   "metadata": {},
   "source": [
    "This notebook provides an end-to-end exploratory data analysis (EDA) pipeline for the Walmart retail sales dataset used in this project.\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "1. Load the raw dataset directly from the project structure.\n",
    "2. Audit data quality and understand the distribution of key fields.\n",
    "3. Engineer time-aware features that improve downstream forecasting models.\n",
    "4. Generate exploratory visualizations to surface seasonal and store-level trends.\n",
    "5. Persist an enriched feature set back into the project `processed/` directory.\n",
    "\n",
    "> ℹ️ This notebook is designed to run from either the repository root or within the Walmart project folder. It assumes the virtual environment in `.venv/` is active so the dependencies declared in `requirements.txt` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e228e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "def resolve_project_dir(project_name: str = \"retail_sales_walmart\") -> Path:\n",
    "    # Locate the Walmart project directory regardless of the launch location.\n",
    "    # The search checks common entry points: repo root, project root, or notebooks folder.\n",
    "    cwd = Path.cwd().resolve()\n",
    "\n",
    "    candidate = cwd\n",
    "    if (candidate / \"projects\" / project_name).exists():\n",
    "        return candidate / \"projects\" / project_name\n",
    "\n",
    "    if candidate.name == project_name and (candidate / \"data\").exists():\n",
    "        return candidate\n",
    "\n",
    "    if candidate.name == \"notebooks\" and (candidate.parent / \"data\").exists():\n",
    "        return candidate.parent\n",
    "\n",
    "    for parent in candidate.parents:\n",
    "        if (parent / \"projects\" / project_name).exists():\n",
    "            return parent / \"projects\" / project_name\n",
    "        if parent.name == project_name and (parent / \"data\").exists():\n",
    "            return parent\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Unable to locate project directory. Please run the notebook from the repo root or \"\n",
    "        \"ensure the current working directory contains the retail_sales_walmart project.\"\n",
    "    )\n",
    "\n",
    "PROJECT_DIR: Final[Path] = resolve_project_dir()\n",
    "DATA_DIR: Final[Path] = PROJECT_DIR / \"data\"\n",
    "RAW_DATA_PATH: Final[Path] = DATA_DIR / \"raw\" / \"Walmart.csv\"\n",
    "PROCESSED_DIR: Final[Path] = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print(f\"Using data source: {RAW_DATA_PATH.relative_to(PROJECT_DIR)}\")\n",
    "print(f\"Processed outputs will be stored under: {PROCESSED_DIR.relative_to(PROJECT_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f419aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d4ec7",
   "metadata": {},
   "source": [
    "## 1. Data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count, col_count = df_raw.shape\n",
    "print(f\"Rows: {row_count:,} | Columns: {col_count}\")\n",
    "\n",
    "display(df_raw.describe(include='all').transpose())\n",
    "\n",
    "missing_summary = (\n",
    "    df_raw.isna()\n",
    "    .sum()\n",
    "    .rename(\"missing_count\")\n",
    "    .to_frame()\n",
    "    .assign(missing_pct=lambda d: (d[\"missing_count\"] / row_count) * 100)\n",
    ")\n",
    "display(missing_summary)\n",
    "\n",
    "duplicates = df_raw.duplicated(subset=[\"Store\", \"Date\"]).sum()\n",
    "print(f\"Duplicate [Store, Date] rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ecc06",
   "metadata": {},
   "source": [
    "## 2. Baseline feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb55a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d-%m-%Y\")\n",
    "df.sort_values([\"Store\", \"Date\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df[\"Holiday_Flag\"] = df[\"Holiday_Flag\"].astype(bool)\n",
    "df[\"Week\"] = df[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "df[\"Month\"] = df[\"Date\"].dt.month\n",
    "df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
    "df[\"DayOfYear\"] = df[\"Date\"].dt.dayofyear\n",
    "\n",
    "# Seasonal encodings\n",
    "df[\"Week_sin\"] = np.sin(2 * math.pi * df[\"Week\"] / 52)\n",
    "df[\"Week_cos\"] = np.cos(2 * math.pi * df[\"Week\"] / 52)\n",
    "df[\"Month_sin\"] = np.sin(2 * math.pi * df[\"Month\"] / 12)\n",
    "df[\"Month_cos\"] = np.cos(2 * math.pi * df[\"Month\"] / 12)\n",
    "\n",
    "# Climate conversions\n",
    "df[\"Temperature_C\"] = (df[\"Temperature\"] - 32) * 5.0 / 9.0\n",
    "df[\"Fuel_Price_Log\"] = np.log1p(df[\"Fuel_Price\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b67c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store-level aggregates\n",
    "store_aggregates = (\n",
    "    df.groupby(\"Store\")[\"Weekly_Sales\"]\n",
    "    .agg([\"mean\", \"median\", \"std\", \"max\", \"min\"])\n",
    "    .rename(columns=lambda c: f\"Weekly_Sales_{c}\")\n",
    ")\n",
    "\n",
    "df = df.merge(store_aggregates, on=\"Store\", how=\"left\")\n",
    "display(store_aggregates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d964f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag and rolling statistics per store\n",
    "lag_features = [1, 2, 4, 13]  # weekly lags (1 week, 2 weeks, monthly, quarterly)\n",
    "rolling_windows = [4, 8, 13]  # monthly, two-month, quarterly\n",
    "\n",
    "for lag in lag_features:\n",
    "    df[f\"Weekly_Sales_lag{lag}\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
    "\n",
    "for window in rolling_windows:\n",
    "    df[f\"Weekly_Sales_ma{window}\"] = (\n",
    "        df.groupby(\"Store\")[\"Weekly_Sales\"]\n",
    "        .transform(lambda s: s.shift(1).rolling(window=window).mean())\n",
    "    )\n",
    "    df[f\"Weekly_Sales_std{window}\"] = (\n",
    "        df.groupby(\"Store\")[\"Weekly_Sales\"]\n",
    "        .transform(lambda s: s.shift(1).rolling(window=window).std())\n",
    "    )\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170e1d6",
   "metadata": {},
   "source": [
    "## 3. Exploratory visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f832676",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lineplot(data=df, x=\"Date\", y=\"Weekly_Sales\", hue=\"Store\", legend=False)\n",
    "plt.title(\"Weekly sales per store\")\n",
    "plt.ylabel(\"Weekly Sales (USD)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary = (\n",
    "    df.groupby(\"Store\")[\"Weekly_Sales\"]\n",
    "    .agg([\"mean\", \"median\", \"std\", \"max\"])\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "display(store_summary.head(10))\n",
    "\n",
    "_ = sns.barplot(\n",
    "    x=store_summary.index,\n",
    "    y=store_summary[\"mean\"],\n",
    "    order=store_summary.index,\n",
    "    palette=\"crest\"\n",
    ")\n",
    "plt.title(\"Average weekly sales by store\")\n",
    "plt.xlabel(\"Store\")\n",
    "plt.ylabel(\"Average Weekly Sales (USD)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = [\n",
    "    \"Weekly_Sales\",\n",
    "    \"Temperature\",\n",
    "    \"Fuel_Price\",\n",
    "    \"CPI\",\n",
    "    \"Unemployment\",\n",
    "    \"Week\",\n",
    "    \"Month\",\n",
    "    \"Quarter\",\n",
    "    \"Weekly_Sales_lag1\",\n",
    "    \"Weekly_Sales_ma4\",\n",
    "]\n",
    "\n",
    "corr_matrix = df[corr_features].dropna().corr()\n",
    "_ = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation heatmap for key drivers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf3d7d",
   "metadata": {},
   "source": [
    "## 4. Save engineered feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.copy()\n",
    "df_features.sort_values([\"Store\", \"Date\"], inplace=True)\n",
    "\n",
    "OUTPUT_PATH = PROCESSED_DIR / \"walmart_features.parquet\"\n",
    "df_features.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved engineered features to {OUTPUT_PATH.relative_to(PROJECT_DIR)}\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541c35f",
   "metadata": {},
   "source": [
    "## 5. Next steps\n",
    "\n",
    "- Feed `walmart_features.parquet` into the Hydra training pipeline (see `src/ml_portfolio/conf/dataset/walmart.yaml`).\n",
    "- Experiment with additional covariates (holiday calendars, promotional events) and re-run this notebook.\n",
    "- Consider scaling features (e.g., standardization) in the training pipeline using the engineered columns created here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
